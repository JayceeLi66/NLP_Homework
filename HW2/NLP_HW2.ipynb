{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "NLP_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayceeLi66/NLP_Homework/blob/master/NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_7bEXDVFSKn",
        "colab_type": "code",
        "outputId": "9e1a632a-5013-4cfa-c22c-c1c693c34e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeoDyeEHFD17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esyPhxXHFD15",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6T6gTiVFD19",
        "colab_type": "text"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjVWreNpFD1-",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKlNuTieFD1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD6SlgoKdWxp",
        "colab_type": "code",
        "outputId": "a51cb10b-b948-46a1-fad4-5bf3033b534a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-30 18:36:20--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 107.152.24.197, 107.152.25.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|107.152.24.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-09-30 18:36:20--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-09-30 18:36:20--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 107.152.24.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|107.152.24.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!NsXapsXEu67UDujCK1vvl7EPyKt7tzqr1Rn3O46tzYEuRY0QrQ4Qg3srXpv49W6MIK4h9P8pR3-LlkHrotTY68h2rBZm8bZK7jqmdYM4o0f0ig1F8Vj4xY4uZg3ra3o-ZXgpt29VF7jsUrLC0j_5hAE0etWPcuheUpwwJODIZI9I8i9eeOdH5LGAQIwpMEBVWQDerOv-CpIPDMCMSTcicfrdbQnkEFnYvfjItf3zWk2c8w7XDsPrV7SC7uSQNTR72W0k_9iDaFuTpaxkytlgLOnRPuQAUckrK2whRPcxoqDVqNwfcjmojY2IkzzpC0AZd6C78N7TyYeWBJUs7ECxEQGx_4uILtIE_XcssNuJ-xIbuuJICniTbEJstjuOECwILXc_xjhCO9aOjycOl8Xo5fbUthKIYyBqqrrhTU7Z-vr4Pd9z0v7kLnrPHChAbQaPepGoZnSvhX3VnFt4Ln4Pv6jNQR8eI9Kv4i7I6IatKkU1s0h0jfkEetsTXaLZf5v_lEUwzglT2hXBmZ1bPfDsxf27-_iX5irfm0l6A5YgV_cX5Llpcr0iN_O4cgi5OpNnu7wjMF1ltIZ2srbmBR4royKrHE4WIsoEidqWnLoQi1duHJ8H8eCajUxriVAta7ds1G3nHGLY9b-uGjFD9vC5QIlQOcmIl8rho_IHS3EsjrJNzg_s68oaJSFFEFdnrhyDErGVVPED_B7CBCfa7RvxAdEZ2pR2MdmS3DxO06LR_jdzGHrq8wfU7zVEwNPBCGGG_5r_zBvXtRVueJwf_ybVcKIGUdfaJ6DDFMYroA0QIqiveZX2lMg6nPF_PkcVF_YQAfQMYk5R1mhC92z4Y0OT1gRSZnsef4_zwnlM2tre-zrgAn6tjfuG7JViT0EJHjBT8Pzz2uv7tgxwGtZxOk4aBZWuPtCdlsCPBblwzMAqZBu07s0UFu59LZ7ZHkiNS1ckEwDH8DDw-LdvgV5U-cFYFTzClU8gDfZkqieHSsQl6CwFYtE7ZTKgCGSikqYkGQb8Mrx7PNOOJyaeoLwtCXyjiXfMG_s1rfv6qF_-NQXJVirAz09OOlk5O5w_swa6-dS7lOzxt25IenYkYbRCzvRtxWl55oaohIkoFYaRgxJ8dMjah67y3w05qjNqwPTWRPzfvHhLzsODmsBJuH9KOTLlXVOm0hrRnLJgwYzkMXwPn9DS2u72Z4hOUpCCo31EU4Z49Czo_dDulWQB_LZrNm1lqeC9qvMtNrAwzPAPmYUo74wmX5SauI_dnM2YzboV1xdiKtYoC6GCkRzOzqDgaSQUFbfhFh7Glyjx8dRZ8f4BmfpmiuqTuLzKPEfWWagQrFPO4Tq4MMrTWjOk__tocEzI2PwjrJpf8kzb1ktl7V-AjLIn4z2lzygPbxUlY9LQq-ajzrbIdzE0I5wZCWlR/download [following]\n",
            "--2019-09-30 18:36:20--  https://public.boxcloud.com/d/1/b1!NsXapsXEu67UDujCK1vvl7EPyKt7tzqr1Rn3O46tzYEuRY0QrQ4Qg3srXpv49W6MIK4h9P8pR3-LlkHrotTY68h2rBZm8bZK7jqmdYM4o0f0ig1F8Vj4xY4uZg3ra3o-ZXgpt29VF7jsUrLC0j_5hAE0etWPcuheUpwwJODIZI9I8i9eeOdH5LGAQIwpMEBVWQDerOv-CpIPDMCMSTcicfrdbQnkEFnYvfjItf3zWk2c8w7XDsPrV7SC7uSQNTR72W0k_9iDaFuTpaxkytlgLOnRPuQAUckrK2whRPcxoqDVqNwfcjmojY2IkzzpC0AZd6C78N7TyYeWBJUs7ECxEQGx_4uILtIE_XcssNuJ-xIbuuJICniTbEJstjuOECwILXc_xjhCO9aOjycOl8Xo5fbUthKIYyBqqrrhTU7Z-vr4Pd9z0v7kLnrPHChAbQaPepGoZnSvhX3VnFt4Ln4Pv6jNQR8eI9Kv4i7I6IatKkU1s0h0jfkEetsTXaLZf5v_lEUwzglT2hXBmZ1bPfDsxf27-_iX5irfm0l6A5YgV_cX5Llpcr0iN_O4cgi5OpNnu7wjMF1ltIZ2srbmBR4royKrHE4WIsoEidqWnLoQi1duHJ8H8eCajUxriVAta7ds1G3nHGLY9b-uGjFD9vC5QIlQOcmIl8rho_IHS3EsjrJNzg_s68oaJSFFEFdnrhyDErGVVPED_B7CBCfa7RvxAdEZ2pR2MdmS3DxO06LR_jdzGHrq8wfU7zVEwNPBCGGG_5r_zBvXtRVueJwf_ybVcKIGUdfaJ6DDFMYroA0QIqiveZX2lMg6nPF_PkcVF_YQAfQMYk5R1mhC92z4Y0OT1gRSZnsef4_zwnlM2tre-zrgAn6tjfuG7JViT0EJHjBT8Pzz2uv7tgxwGtZxOk4aBZWuPtCdlsCPBblwzMAqZBu07s0UFu59LZ7ZHkiNS1ckEwDH8DDw-LdvgV5U-cFYFTzClU8gDfZkqieHSsQl6CwFYtE7ZTKgCGSikqYkGQb8Mrx7PNOOJyaeoLwtCXyjiXfMG_s1rfv6qF_-NQXJVirAz09OOlk5O5w_swa6-dS7lOzxt25IenYkYbRCzvRtxWl55oaohIkoFYaRgxJ8dMjah67y3w05qjNqwPTWRPzfvHhLzsODmsBJuH9KOTLlXVOm0hrRnLJgwYzkMXwPn9DS2u72Z4hOUpCCo31EU4Z49Czo_dDulWQB_LZrNm1lqeC9qvMtNrAwzPAPmYUo74wmX5SauI_dnM2YzboV1xdiKtYoC6GCkRzOzqDgaSQUFbfhFh7Glyjx8dRZ8f4BmfpmiuqTuLzKPEfWWagQrFPO4Tq4MMrTWjOk__tocEzI2PwjrJpf8kzb1ktl7V-AjLIn4z2lzygPbxUlY9LQq-ajzrbIdzE0I5wZCWlR/download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.27.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.27.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  16.2MB/s    in 0.7s    \n",
            "\n",
            "2019-09-30 18:36:22 (16.2 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n",
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYTQ-YsyFD2B",
        "colab_type": "text"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw3IkAhVFD2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZauFuH0uFD2D",
        "colab_type": "code",
        "outputId": "78bd714d-cb6e-41cc-eaaf-92e21e226ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTl8YMzwFD2G",
        "colab_type": "text"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJA2055PFD2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    def __init__(self, lambda_ls, vsize):\n",
        "        self.l = lambda_ls\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for n in range(1, len(self.l)):\n",
        "            for sequence in sequences:\n",
        "                padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "                for i in range(len(padded_sequence) - n+1):\n",
        "                    ngram = tuple(padded_sequence[i:i+n])\n",
        "                    prefix, word = ngram[:-1], ngram[-1]\n",
        "                    self.count[prefix][word] += 1\n",
        "                    self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        n = len(self.l)\n",
        "        padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - n):\n",
        "            ngram = tuple(padded_sequence[i:i+n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prob = self.l[0]/self.vsize\n",
        "        for n in range(1, len(self.l)):\n",
        "            prefix = ngram[-n:-1]\n",
        "            word = ngram[-1]\n",
        "            if self.total[prefix] != 0:\n",
        "                prob += self.l[n] * self.count[prefix][word] / self.total[prefix]\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp18DuEoUttV",
        "colab_type": "text"
      },
      "source": [
        "#### Tune $\\lambda$ for n=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1LjDsXUPUNE",
        "colab_type": "code",
        "outputId": "e00c1545-20df-4f5e-d12b-b1fb3a04e5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "lambda_range = [[0.33, 0.33, 0.34], [0.2, 0.3, 0.5], \n",
        "                [0.2, 0.8, 0], [0.2, 0, 0.8],\n",
        "                [0.1, 0.1, 0.8]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=2, lambda=[0.33, 0.33, 0.34], Train Perplexity: 167.716\n",
            "Interpolation smoothing, n=2, lambda=[0.33, 0.33, 0.34], Valid Perplexity: 370.320\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0.3, 0.5], Train Perplexity: 126.119\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0.3, 0.5], Valid Perplexity: 316.744\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0.8, 0], Train Perplexity: 1060.071\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0.8, 0], Valid Perplexity: 907.352\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0, 0.8], Train Perplexity: 92.656\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0, 0.8], Valid Perplexity: 420.348\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.1, 0.8], Train Perplexity: 90.183\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.1, 0.8], Valid Perplexity: 307.340\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ll1GtXui82H",
        "colab_type": "code",
        "outputId": "44d79d8d-6a5d-4a10-e81f-69a0d916678c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "lambda_range = [[0.1, 0.2, 0.7], [0.1, 0.3, 0.6],\n",
        "                [0.01, 0.29, 0.7], [0.01, 0.2, 0.79]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=2, lambda=[0.1, 0.2, 0.7], Train Perplexity: 98.576\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.2, 0.7], Valid Perplexity: 295.600\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.3, 0.6], Train Perplexity: 109.225\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.3, 0.6], Valid Perplexity: 297.180\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.01, 0.29, 0.7], Train Perplexity: 96.745\n",
            "Interpolation smoothing, n=2, lambda=[0.01, 0.29, 0.7], Valid Perplexity: 294.042\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.01, 0.2, 0.79], Train Perplexity: 89.199\n",
            "Interpolation smoothing, n=2, lambda=[0.01, 0.2, 0.79], Valid Perplexity: 300.005\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLY7dDfvky-Q",
        "colab_type": "text"
      },
      "source": [
        "Best setting: [0.01, 0.29, 0.7]\n",
        "\n",
        "Lowest Valid Perplexity: 294.042"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5op9lgxVPoW",
        "colab_type": "text"
      },
      "source": [
        "#### Tune $\\lambda$ for n=3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOGTNznQVP_I",
        "colab_type": "code",
        "outputId": "c96ce373-d496-40d6-90e4-e3d19426acc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.3, 0.4], [0.01, 0.29, 0.2, 0.5],\n",
        "                [0.01, 0.19, 0.3, 0.5], [0.01, 0.19, 0.2, 0.6]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.3, 0.4], Train Perplexity: 15.325\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.3, 0.4], Valid Perplexity: 293.228\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.2, 0.5], Train Perplexity: 13.462\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.2, 0.5], Valid Perplexity: 318.350\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.3, 0.5], Train Perplexity: 12.978\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.3, 0.5], Valid Perplexity: 313.347\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.2, 0.6], Train Perplexity: 11.655\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.2, 0.6], Valid Perplexity: 344.486\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxFaL--HlwCu",
        "colab_type": "code",
        "outputId": "429004f9-60ba-410b-bc8c-9616d7e04617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.4, 0.3], [0.01, 0.29, 0.5, 0.2],\n",
        "                [0.01, 0.19, 0.5, 0.3], [0.01, 0.19, 0.6, 0.2]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.4, 0.3], Train Perplexity: 17.985\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.4, 0.3], Valid Perplexity: 278.743\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.5, 0.2], Train Perplexity: 22.212\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.5, 0.2], Valid Perplexity: 271.353\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.5, 0.3], Train Perplexity: 17.256\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.5, 0.3], Valid Perplexity: 283.224\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.6, 0.2], Train Perplexity: 21.223\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.6, 0.2], Valid Perplexity: 277.553\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMdrNcZImqz7",
        "colab_type": "text"
      },
      "source": [
        "Best setting: [0.01, 0.29, 0.5, 0.2]\n",
        "\n",
        "Lowest Valid Perplexity: 271.353"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbrftUztVXfl",
        "colab_type": "text"
      },
      "source": [
        "#### Tune $\\lambda$ for n=4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwMScL62VbO1",
        "colab_type": "code",
        "outputId": "d107441c-b088-4ef6-8635-f61ef8db0c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.5, 0.1, 0.1], [0.01, 0.29, 0.4, 0.2, 0.1],\n",
        "                [0.01, 0.19, 0.5, 0.2, 0.1]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.1, 0.1], Train Perplexity: 10.469\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.1, 0.1], Valid Perplexity: 277.409\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.4, 0.2, 0.1], Train Perplexity: 9.083\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.4, 0.2, 0.1], Valid Perplexity: 282.617\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.19, 0.5, 0.2, 0.1], Train Perplexity: 8.805\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.19, 0.5, 0.2, 0.1], Valid Perplexity: 286.733\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY2ndRMBoXfg",
        "colab_type": "code",
        "outputId": "87954f3b-9b83-415c-cd93-488b1f46f111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.5, 0.19, 0.01], [0.01, 0.29, 0.49, 0.2, 0.01],\n",
        "                [0.01, 0.28, 0.5, 0.2, 0.01]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.19, 0.01], Train Perplexity: 17.175\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.19, 0.01], Valid Perplexity: 270.364\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.2, 0.01], Train Perplexity: 16.820\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.2, 0.01], Valid Perplexity: 270.813\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.5, 0.2, 0.01], Train Perplexity: 16.750\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.5, 0.2, 0.01], Valid Perplexity: 270.926\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQZaH3DTo5Sw",
        "colab_type": "code",
        "outputId": "51b363cf-2677-4d15-8045-73ce81d704f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.49, 0.19, 0.02], [0.01, 0.28, 0.49, 0.19, 0.03]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.19, 0.02], Train Perplexity: 15.120\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.19, 0.02], Valid Perplexity: 270.873\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.49, 0.19, 0.03], Train Perplexity: 13.689\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.49, 0.19, 0.03], Valid Perplexity: 271.795\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w1shZgupWKq",
        "colab_type": "text"
      },
      "source": [
        "Best setting: [0.01, 0.29, 0.5, 0.19, 0.01]\n",
        "\n",
        "Lowest Valid Perplexity: 270.364"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsyZcX5NFD2I",
        "colab_type": "text"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXbg0_GLFD2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKRpKvHPFD2K",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I_ULEaoFD2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import RNNCell\n",
        "from torch.nn import RNNBase, RNN\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import Embedding\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import sys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bgOuOZwFD2M",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4Z4ZOHuFD2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-H2LVI9hQNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
        "\n",
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    pad_token = wiki_dict.get_id('<pad>')\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5H5Cby2hWMS",
        "colab_type": "code",
        "outputId": "9a7b2233-7c94-46c5-d3cf-93369950469d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "wiki_dict = Dictionary(datasets)\n",
        "wiki_tokenized_datasets = tokenize_dataset(datasets, wiki_dict)\n",
        "wiki_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wiki_tokenized_datasets.items():\n",
        "    wiki_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "wiki_loaders = {}\n",
        "batch_size = 128\n",
        "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
        "    wiki_loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 110962.04it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 119911.33it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 116491.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djvpCg40gYw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs = self.rnn(embeddings)\n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdGXmLhxgqNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_pretrained = False\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if load_pretrained:\n",
        "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
        "        raise EOFError('Download pretrained model!')\n",
        "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
        "    \n",
        "    options = model_dict['options']\n",
        "    model = RNNLanguageModel(options).to(current_device)\n",
        "    model.load_state_dict(model_dict['model_dict'])\n",
        "    \n",
        "else:\n",
        "    embedding_size = 256\n",
        "    hidden_size = 512\n",
        "    num_layers = 3\n",
        "    rnn_dropout = 0.3\n",
        "\n",
        "    options = {\n",
        "        'num_embeddings': len(wiki_dict),\n",
        "        'embedding_dim': embedding_size,\n",
        "        'padding_idx': wiki_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_size,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'rnn_dropout': rnn_dropout,\n",
        "    }\n",
        "\n",
        "    \n",
        "\n",
        "    model = RNNLanguageModel(options).to(current_device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(model_parameters, lr=0.001, momentum=0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpG6FnfxiQSB",
        "colab_type": "code",
        "outputId": "d89b561c-5c1c-4385-c936-d030b8a16ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "plot_cache = []\n",
        "\n",
        "for epoch_number in range(2):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        # do train\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(wiki_loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 10.4196\n",
            "Step 100 avg train loss = 10.1666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYq80CJWiQP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYSozgT-FD2O",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ_UFDZ7FD2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], batch_first=True)\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        lstm_outputs = self.lstm(embeddings)\n",
        "        logits = self.projection(lstm_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGpTDBhXbiX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 3\n",
        "\n",
        "options = {\n",
        "    'num_embeddings': len(wiki_dict),\n",
        "    'embedding_dim': embedding_size,\n",
        "    'padding_idx': wiki_dict.get_id('<pad>'),\n",
        "    'input_size': embedding_size,\n",
        "    'hidden_size': hidden_size,\n",
        "    'num_layers': num_layers,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "model = LSTMLanguageModel(options).to('cpu')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(model_parameters, lr=0.001, momentum=0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGfcKJInFD2Q",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHAhniSrFD2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtStk2HTFD2T",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuoRjCbGFD2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dUp76rbFD2V",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEPytwW7FD2V",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEZpRBmjFD2W",
        "colab_type": "code",
        "outputId": "cc81cd68-1f7d-4e8d-b6ec-b6cfe85e988b",
        "colab": {}
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNBmEuA9FD2Y",
        "colab_type": "code",
        "outputId": "443f6eea-071d-46bc-d5dc-e4f2eb7dcf85",
        "colab": {}
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnFZ2mWrFD2a",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDY1LxZjFD2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9yLPROjFD2c",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ajV_nE3FD2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnLN38OVFD2e",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w3jcuqWFD2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzzpebFkFD2h",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhQmqQXAFD2i",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27_vDpScFD2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oop65tFhFD2l",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPsJQM-bFD2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4MgROaRFD2o",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftL6eIPwFD2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDvxYZBtFD2r",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFSKZbkOFD2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAnPOZzvFD2x",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKKkDVtkFD2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owgPfDU3FD2z",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaQZig44FD2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}