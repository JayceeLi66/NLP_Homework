{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JayceeLi66/NLP_Homework/blob/master/NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "H_7bEXDVFSKn",
    "outputId": "9e1a632a-5013-4cfa-c22c-c1c693c34e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WeoDyeEHFD17"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esyPhxXHFD15"
   },
   "source": [
    "# DS-GA 1011 Homework 2\n",
    "## N-Gram and Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6T6gTiVFD19"
   },
   "source": [
    "## I. N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjVWreNpFD1-"
   },
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKlNuTieFD1_"
   },
   "outputs": [],
   "source": [
    "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
    "    if not os.path.exists(filename):\n",
    "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
    "    \n",
    "    datasets = json.load(open(filename, 'r'))\n",
    "    for name in datasets:\n",
    "        datasets[name] = [x.split() for x in datasets[name]]\n",
    "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
    "    print(\"Vocab size: %d\" % (len(vocab)))\n",
    "    return datasets, vocab\n",
    "\n",
    "def perplexity(model, sequences):\n",
    "    n_total = 0\n",
    "    logp_total = 0\n",
    "    for sequence in sequences:\n",
    "        logp_total += model.sequence_logp(sequence)\n",
    "        n_total += len(sequence) + 1  \n",
    "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "GD6SlgoKdWxp",
    "outputId": "a51cb10b-b948-46a1-fad4-5bf3033b534a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 33175\n"
     ]
    }
   ],
   "source": [
    "datasets, vocab = load_wikitext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYTQ-YsyFD2B"
   },
   "source": [
    "### Additive Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbrftUztVXfl"
   },
   "source": [
    "#### Tune $\\lambda$ for n=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "KwMScL62VbO1",
    "outputId": "d107441c-b088-4ef6-8635-f61ef8db0c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.1, 0.1], Train Perplexity: 10.469\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.1, 0.1], Valid Perplexity: 277.409\n",
      "\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.4, 0.2, 0.1], Train Perplexity: 9.083\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.4, 0.2, 0.1], Valid Perplexity: 282.617\n",
      "\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.19, 0.5, 0.2, 0.1], Train Perplexity: 8.805\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.19, 0.5, 0.2, 0.1], Valid Perplexity: 286.733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_range = [[0.01, 0.29, 0.5, 0.1, 0.1], [0.01, 0.29, 0.4, 0.2, 0.1],\n",
    "                [0.01, 0.19, 0.5, 0.2, 0.1]]\n",
    "for l in lambda_range:\n",
    "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
    "    lm.estimate(datasets['train'])\n",
    "\n",
    "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
    "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "SY2ndRMBoXfg",
    "outputId": "87954f3b-9b83-415c-cd93-488b1f46f111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.19, 0.01], Train Perplexity: 17.175\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.19, 0.01], Valid Perplexity: 270.364\n",
      "\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.2, 0.01], Train Perplexity: 16.820\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.2, 0.01], Valid Perplexity: 270.813\n",
      "\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.5, 0.2, 0.01], Train Perplexity: 16.750\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.5, 0.2, 0.01], Valid Perplexity: 270.926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_range = [[0.01, 0.29, 0.5, 0.19, 0.01], [0.01, 0.29, 0.49, 0.2, 0.01],\n",
    "                [0.01, 0.28, 0.5, 0.2, 0.01]]\n",
    "for l in lambda_range:\n",
    "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
    "    lm.estimate(datasets['train'])\n",
    "\n",
    "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
    "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "CQZaH3DTo5Sw",
    "outputId": "51b363cf-2677-4d15-8045-73ce81d704f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.19, 0.02], Train Perplexity: 15.120\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.19, 0.02], Valid Perplexity: 270.873\n",
      "\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.49, 0.19, 0.03], Train Perplexity: 13.689\n",
      "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.49, 0.19, 0.03], Valid Perplexity: 271.795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_range = [[0.01, 0.29, 0.49, 0.19, 0.02], [0.01, 0.28, 0.49, 0.19, 0.03]]\n",
    "for l in lambda_range:\n",
    "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
    "    lm.estimate(datasets['train'])\n",
    "\n",
    "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
    "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6w1shZgupWKq"
   },
   "source": [
    "Best setting: [0.01, 0.29, 0.5, 0.19, 0.01]\n",
    "\n",
    "Lowest Valid Perplexity: 270.364"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsyZcX5NFD2I"
   },
   "source": [
    "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXbg0_GLFD2I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKRpKvHPFD2K"
   },
   "source": [
    "## II. Neural Language Modeling with a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0I_ULEaoFD2L"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import RNNCell\n",
    "from torch.nn import RNNBase, RNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Embedding\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bgOuOZwFD2M"
   },
   "source": [
    "#### Utilities\n",
    "\n",
    "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4Z4ZOHuFD2N"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, datasets, include_valid=False):\n",
    "        self.tokens = []\n",
    "        self.ids = {}\n",
    "        self.counts = {}\n",
    "        self.add_token('<bos>')\n",
    "        self.add_token('<eos>')\n",
    "        self.add_token('<pad>')\n",
    "        self.add_token('<unk>')\n",
    "        \n",
    "        for line in tqdm(datasets['train']):\n",
    "            for w in line:\n",
    "                self.add_token(w)\n",
    "                    \n",
    "        if include_valid is True:\n",
    "            for line in tqdm(datasets['valid']):\n",
    "                for w in line:\n",
    "                    self.add_token(w)\n",
    "                            \n",
    "    def add_token(self, w):\n",
    "        if w not in self.tokens:\n",
    "            self.tokens.append(w)\n",
    "            _w_id = len(self.tokens) - 1\n",
    "            self.ids[w] = _w_id\n",
    "            self.counts[w] = 1\n",
    "        else:\n",
    "            self.counts[w] += 1\n",
    "\n",
    "    def get_id(self, w):\n",
    "        return self.ids[w]\n",
    "    \n",
    "    def get_token(self, idx):\n",
    "        return self.tokens[idx]\n",
    "    \n",
    "    def decode_idx_seq(self, l):\n",
    "        return [self.tokens[i] for i in l]\n",
    "    \n",
    "    def encode_token_seq(self, l):\n",
    "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
    "    tokenized_datasets = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        _current_dictified = []\n",
    "        for l in tqdm(dataset):\n",
    "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
    "            encoded_l = dictionary.encode_token_seq(l)\n",
    "            _current_dictified.append(encoded_l)\n",
    "        tokenized_datasets[split] = _current_dictified\n",
    "        \n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-H2LVI9hQNF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
    "\n",
    "class TensoredDataset(Dataset):\n",
    "    def __init__(self, list_of_lists_of_tokens):\n",
    "        self.input_tensors = []\n",
    "        self.target_tensors = []\n",
    "        \n",
    "        for sample in list_of_lists_of_tokens:\n",
    "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
    "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a (input, target) tuple\n",
    "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
    "\n",
    "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
    "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
    "    padded_list = []\n",
    "    \n",
    "    for t in list_of_tensors:\n",
    "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
    "        padded_list.append(padded_tensor)\n",
    "        \n",
    "    padded_tensor = torch.cat(padded_list, dim=0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    # batch is a list of sample tuples\n",
    "    input_list = [s[0] for s in batch]\n",
    "    target_list = [s[1] for s in batch]\n",
    "    \n",
    "    pad_token = wiki_dict.get_id('<pad>')\n",
    "    \n",
    "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
    "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
    "    \n",
    "    return input_tensor, target_tensor      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "s5H5Cby2hWMS",
    "outputId": "9a7b2233-7c94-46c5-d3cf-93369950469d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78274/78274 [00:00<00:00, 110962.04it/s]\n",
      "100%|██████████| 8464/8464 [00:00<00:00, 119911.33it/s]\n",
      "100%|██████████| 9708/9708 [00:00<00:00, 116491.78it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_dict = Dictionary(datasets)\n",
    "wiki_tokenized_datasets = tokenize_dataset(datasets, wiki_dict)\n",
    "wiki_tensor_dataset = {}\n",
    "\n",
    "for split, listoflists in wiki_tokenized_datasets.items():\n",
    "    wiki_tensor_dataset[split] = TensoredDataset(listoflists)\n",
    "    \n",
    "wiki_loaders = {}\n",
    "batch_size = 128\n",
    "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
    "    wiki_loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djvpCg40gYw6"
   },
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        rnn_outputs = self.rnn(embeddings)\n",
    "        logits = self.projection(rnn_outputs[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdGXmLhxgqNg"
   },
   "outputs": [],
   "source": [
    "load_pretrained = False\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    current_device = 'cuda'\n",
    "else:\n",
    "    current_device = 'cpu'\n",
    "\n",
    "if load_pretrained:\n",
    "    if not os.path.exists('personachat_rnn_lm.pt'):\n",
    "        raise EOFError('Download pretrained model!')\n",
    "    model_dict = torch.load('personachat_rnn_lm.pt')\n",
    "    \n",
    "    options = model_dict['options']\n",
    "    model = RNNLanguageModel(options).to(current_device)\n",
    "    model.load_state_dict(model_dict['model_dict'])\n",
    "    \n",
    "else:\n",
    "    embedding_size = 256\n",
    "    hidden_size = 512\n",
    "    num_layers = 3\n",
    "    rnn_dropout = 0.3\n",
    "\n",
    "    options = {\n",
    "        'num_embeddings': len(wiki_dict),\n",
    "        'embedding_dim': embedding_size,\n",
    "        'padding_idx': wiki_dict.get_id('<pad>'),\n",
    "        'input_size': embedding_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'rnn_dropout': rnn_dropout,\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    model = RNNLanguageModel(options).to(current_device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(model_parameters, lr=0.001, momentum=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "LpG6FnfxiQSB",
    "outputId": "d89b561c-5c1c-4385-c936-d030b8a16ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 avg train loss = 10.4196\n",
      "Step 100 avg train loss = 10.1666\n"
     ]
    }
   ],
   "source": [
    "plot_cache = []\n",
    "\n",
    "for epoch_number in range(2):\n",
    "    avg_loss=0\n",
    "    if not load_pretrained:\n",
    "        # do train\n",
    "        model.train()\n",
    "        train_log_cache = []\n",
    "        for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
    "            optimizer.zero_grad()\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_log_cache.append(loss.item())\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
    "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "                train_log_cache = []\n",
    "            \n",
    "    #do valid\n",
    "    valid_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inp, target) in enumerate(wiki_loaders['valid']):\n",
    "            inp = inp.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            logits = model(inp)\n",
    "\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            valid_losses.append(loss.item())\n",
    "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
    "        \n",
    "    plot_cache.append((avg_loss, avg_val_loss))\n",
    "\n",
    "    if load_pretrained:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYq80CJWiQP8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYSozgT-FD2O"
   },
   "source": [
    "### II.1 LSTM and Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZ_UFDZ7FD2P"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
    "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence)\n",
    "        lstm_outputs = self.lstm(embeddings)\n",
    "        logits = self.projection(lstm_outputs[0])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oGpTDBhXbiX2"
   },
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 3\n",
    "\n",
    "options = {\n",
    "    'num_embeddings': len(wiki_dict),\n",
    "    'embedding_dim': embedding_size,\n",
    "    'padding_idx': wiki_dict.get_id('<pad>'),\n",
    "    'input_size': embedding_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers': num_layers,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "model = LSTMLanguageModel(options).to('cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(model_parameters, lr=0.001, momentum=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = RNNLanguageModel(options).to(current_device)\n",
    "# rnn.load_state_dict(torch.load('RNN_Baseline.ckpt'))\n",
    "# valid_losses = []\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for i, (inp, target) in enumerate(wiki_loaders['valid']):\n",
    "#         inp = inp.to(current_device)\n",
    "#         target = target.to(current_device)\n",
    "#         logits = rnn(inp)\n",
    "\n",
    "#         loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "#         valid_losses.append(loss.item())\n",
    "#     avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "#     ppl_val = 2**(avg_val_loss/np.log(2))\n",
    "#     print('Validation Perplexity = {:.{prec}f}'.format(ppl_val, prec=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGfcKJInFD2Q"
   },
   "source": [
    "#### Results (LSTM vs. Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHAhniSrFD2R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtStk2HTFD2T"
   },
   "source": [
    "#### Performance Variation Based on Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuoRjCbGFD2T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dUp76rbFD2V"
   },
   "source": [
    "### II.2 Learned Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hEPytwW7FD2V"
   },
   "source": [
    "#### Utilities\n",
    "\n",
    "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
    "\n",
    "Use `!pip install umap-learn` to install UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEZpRBmjFD2W",
    "outputId": "cc81cd68-1f7d-4e8d-b6ec-b6cfe85e988b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def umap_plot(weight_matrix, word_ids, words):\n",
    "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
    "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
    "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    to_plot = reduced[word_ids, :]\n",
    "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        current_point = to_plot[i]\n",
    "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNBmEuA9FD2Y",
    "outputId": "443f6eea-071d-46bc-d5dc-e4f2eb7dcf85"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Vsize = 100                                 # e.g. len(dictionary)\n",
    "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
    "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
    "\n",
    "words = ['the', 'dog', 'ran']\n",
    "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
    "\n",
    "umap_plot(fake_weight_matrix, word_ids, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnFZ2mWrFD2a"
   },
   "source": [
    "#### II.2.1 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDY1LxZjFD2a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9yLPROjFD2c"
   },
   "source": [
    "#### II.2.2 Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ajV_nE3FD2c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnLN38OVFD2e"
   },
   "source": [
    "#### II.2.3 Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8w3jcuqWFD2e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzzpebFkFD2h"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhQmqQXAFD2i"
   },
   "source": [
    "### II.3 Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27_vDpScFD2i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oop65tFhFD2l"
   },
   "source": [
    "#### II.3.2 Highest and Lowest scoring sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPsJQM-bFD2m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4MgROaRFD2o"
   },
   "source": [
    "#### II.3.3 Modified sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftL6eIPwFD2p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDvxYZBtFD2r"
   },
   "source": [
    "### II.4 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFSKZbkOFD2s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAnPOZzvFD2x"
   },
   "source": [
    "#### II.4.3 Number of unique tokens and sequence length \n",
    "\n",
    "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKKkDVtkFD2x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owgPfDU3FD2z"
   },
   "source": [
    "#### II.4.4 Example Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaQZig44FD2z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NLP_HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
