{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_HW2_Li.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "V6T6gTiVFD19",
        "zjVWreNpFD1-",
        "tYTQ-YsyFD2B",
        "gTl8YMzwFD2G",
        "2bgOuOZwFD2M",
        "AHb3rdoxCPcH",
        "nHkE4Fvanuvr",
        "uKy5oDovr6Lx",
        "1cJeykS5yO7Q"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H_7bEXDVFSKn",
        "outputId": "32454a13-0807-4d3b-da7d-edc5e86db1c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WeoDyeEHFD17",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "esyPhxXHFD15"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V6T6gTiVFD19"
      },
      "source": [
        "## I. N-Gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zjVWreNpFD1-"
      },
      "source": [
        "#### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HKlNuTieFD1_",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GD6SlgoKdWxp",
        "outputId": "689df7dd-8a65-47c6-f3d1-6c3ff2d16dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tYTQ-YsyFD2B"
      },
      "source": [
        "### Additive Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kw3IkAhVFD2B",
        "colab": {}
      },
      "source": [
        "class NGramAdditive(object):\n",
        "    def __init__(self, n, delta, vsize):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for sequence in sequences:\n",
        "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "            for i in range(len(padded_sequence) - self.n+1):\n",
        "                ngram = tuple(padded_sequence[i:i+self.n])\n",
        "                prefix, word = ngram[:-1], ngram[-1]\n",
        "                self.count[prefix][word] += 1\n",
        "                self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - self.n+1):\n",
        "            ngram = tuple(padded_sequence[i:i+self.n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prefix = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        prob = ((self.delta + self.count[prefix][word]) / \n",
        "                (self.total[prefix] + self.delta*self.vsize))\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZauFuH0uFD2D",
        "outputId": "78bd714d-cb6e-41cc-eaaf-92e21e226ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "delta = 0.0005\n",
        "for n in [2, 3, 4]:\n",
        "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
            "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
            "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
            "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTl8YMzwFD2G"
      },
      "source": [
        "### I.1 Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QJA2055PFD2G",
        "colab": {}
      },
      "source": [
        "class NGramInterpolation(object):\n",
        "    def __init__(self, lambda_ls, vsize):\n",
        "        self.l = lambda_ls\n",
        "        self.count = defaultdict(lambda: defaultdict(float))\n",
        "        self.total = defaultdict(float)\n",
        "        self.vsize = vsize\n",
        "    \n",
        "    def estimate(self, sequences):\n",
        "        for n in range(1, len(self.l)):\n",
        "            for sequence in sequences:\n",
        "                padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "                for i in range(len(padded_sequence) - n+1):\n",
        "                    ngram = tuple(padded_sequence[i:i+n])\n",
        "                    prefix, word = ngram[:-1], ngram[-1]\n",
        "                    self.count[prefix][word] += 1\n",
        "                    self.total[prefix] += 1\n",
        "                \n",
        "    def sequence_logp(self, sequence):\n",
        "        n = len(self.l)\n",
        "        padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
        "        total_logp = 0\n",
        "        for i in range(len(padded_sequence) - n):\n",
        "            ngram = tuple(padded_sequence[i:i+n])\n",
        "            total_logp += np.log2(self.ngram_prob(ngram))\n",
        "        return total_logp\n",
        "\n",
        "\n",
        "\n",
        "    def ngram_prob(self, ngram):\n",
        "        prob = self.l[0]/self.vsize\n",
        "        for n in range(1, len(self.l)):\n",
        "            prefix = ngram[-n:-1]\n",
        "            word = ngram[-1]\n",
        "            if self.total[prefix] != 0:\n",
        "                prob += self.l[n] * self.count[prefix][word] / self.total[prefix]\n",
        "        return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gp18DuEoUttV"
      },
      "source": [
        "#### Tune $\\lambda$ for n=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y1LjDsXUPUNE",
        "outputId": "e00c1545-20df-4f5e-d12b-b1fb3a04e5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "lambda_range = [[0.33, 0.33, 0.34], [0.2, 0.3, 0.5], \n",
        "                [0.2, 0.8, 0], [0.2, 0, 0.8],\n",
        "                [0.1, 0.1, 0.8]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=2, lambda=[0.33, 0.33, 0.34], Train Perplexity: 167.716\n",
            "Interpolation smoothing, n=2, lambda=[0.33, 0.33, 0.34], Valid Perplexity: 370.320\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0.3, 0.5], Train Perplexity: 126.119\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0.3, 0.5], Valid Perplexity: 316.744\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0.8, 0], Train Perplexity: 1060.071\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0.8, 0], Valid Perplexity: 907.352\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0, 0.8], Train Perplexity: 92.656\n",
            "Interpolation smoothing, n=2, lambda=[0.2, 0, 0.8], Valid Perplexity: 420.348\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.1, 0.8], Train Perplexity: 90.183\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.1, 0.8], Valid Perplexity: 307.340\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ll1GtXui82H",
        "outputId": "44d79d8d-6a5d-4a10-e81f-69a0d916678c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "lambda_range = [[0.1, 0.2, 0.7], [0.1, 0.3, 0.6],\n",
        "                [0.01, 0.29, 0.7], [0.01, 0.2, 0.79]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=2, lambda=[0.1, 0.2, 0.7], Train Perplexity: 98.576\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.2, 0.7], Valid Perplexity: 295.600\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.3, 0.6], Train Perplexity: 109.225\n",
            "Interpolation smoothing, n=2, lambda=[0.1, 0.3, 0.6], Valid Perplexity: 297.180\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.01, 0.29, 0.7], Train Perplexity: 96.745\n",
            "Interpolation smoothing, n=2, lambda=[0.01, 0.29, 0.7], Valid Perplexity: 294.042\n",
            "\n",
            "Interpolation smoothing, n=2, lambda=[0.01, 0.2, 0.79], Train Perplexity: 89.199\n",
            "Interpolation smoothing, n=2, lambda=[0.01, 0.2, 0.79], Valid Perplexity: 300.005\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QLY7dDfvky-Q"
      },
      "source": [
        "Best setting: [0.01, 0.29, 0.7]\n",
        "\n",
        "Lowest Valid Perplexity: 294.042"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a5op9lgxVPoW"
      },
      "source": [
        "#### Tune $\\lambda$ for n=3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sOGTNznQVP_I",
        "outputId": "c96ce373-d496-40d6-90e4-e3d19426acc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.3, 0.4], [0.01, 0.29, 0.2, 0.5],\n",
        "                [0.01, 0.19, 0.3, 0.5], [0.01, 0.19, 0.2, 0.6]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.3, 0.4], Train Perplexity: 15.325\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.3, 0.4], Valid Perplexity: 293.228\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.2, 0.5], Train Perplexity: 13.462\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.2, 0.5], Valid Perplexity: 318.350\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.3, 0.5], Train Perplexity: 12.978\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.3, 0.5], Valid Perplexity: 313.347\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.2, 0.6], Train Perplexity: 11.655\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.2, 0.6], Valid Perplexity: 344.486\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zxFaL--HlwCu",
        "outputId": "429004f9-60ba-410b-bc8c-9616d7e04617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.4, 0.3], [0.01, 0.29, 0.5, 0.2],\n",
        "                [0.01, 0.19, 0.5, 0.3], [0.01, 0.19, 0.6, 0.2]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.4, 0.3], Train Perplexity: 17.985\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.4, 0.3], Valid Perplexity: 278.743\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.5, 0.2], Train Perplexity: 22.212\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.29, 0.5, 0.2], Valid Perplexity: 271.353\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.5, 0.3], Train Perplexity: 17.256\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.5, 0.3], Valid Perplexity: 283.224\n",
            "\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.6, 0.2], Train Perplexity: 21.223\n",
            "Interpolation smoothing, n=3, lambda=[0.01, 0.19, 0.6, 0.2], Valid Perplexity: 277.553\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zMdrNcZImqz7"
      },
      "source": [
        "Best setting: [0.01, 0.29, 0.5, 0.2]\n",
        "\n",
        "Lowest Valid Perplexity: 271.353"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YbrftUztVXfl"
      },
      "source": [
        "#### Tune $\\lambda$ for n=4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KwMScL62VbO1",
        "outputId": "d107441c-b088-4ef6-8635-f61ef8db0c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.5, 0.1, 0.1], [0.01, 0.29, 0.4, 0.2, 0.1],\n",
        "                [0.01, 0.19, 0.5, 0.2, 0.1]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.1, 0.1], Train Perplexity: 10.469\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.1, 0.1], Valid Perplexity: 277.409\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.4, 0.2, 0.1], Train Perplexity: 9.083\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.4, 0.2, 0.1], Valid Perplexity: 282.617\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.19, 0.5, 0.2, 0.1], Train Perplexity: 8.805\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.19, 0.5, 0.2, 0.1], Valid Perplexity: 286.733\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SY2ndRMBoXfg",
        "outputId": "87954f3b-9b83-415c-cd93-488b1f46f111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.5, 0.19, 0.01], [0.01, 0.29, 0.49, 0.2, 0.01],\n",
        "                [0.01, 0.28, 0.5, 0.2, 0.01]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.19, 0.01], Train Perplexity: 17.175\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.5, 0.19, 0.01], Valid Perplexity: 270.364\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.2, 0.01], Train Perplexity: 16.820\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.2, 0.01], Valid Perplexity: 270.813\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.5, 0.2, 0.01], Train Perplexity: 16.750\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.5, 0.2, 0.01], Valid Perplexity: 270.926\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CQZaH3DTo5Sw",
        "outputId": "51b363cf-2677-4d15-8045-73ce81d704f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "lambda_range = [[0.01, 0.29, 0.49, 0.19, 0.02], [0.01, 0.28, 0.49, 0.19, 0.03]]\n",
        "for l in lambda_range:\n",
        "    lm = NGramInterpolation(lambda_ls=l, vsize=len(vocab)+1)\n",
        "    lm.estimate(datasets['train'])\n",
        "\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Train Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['train'])))\n",
        "    print(\"Interpolation smoothing, n=%d, lambda=%s, Valid Perplexity: %.3f\" % (len(l)-1, l, perplexity(lm, datasets['valid'])))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.19, 0.02], Train Perplexity: 15.120\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.29, 0.49, 0.19, 0.02], Valid Perplexity: 270.873\n",
            "\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.49, 0.19, 0.03], Train Perplexity: 13.689\n",
            "Interpolation smoothing, n=4, lambda=[0.01, 0.28, 0.49, 0.19, 0.03], Valid Perplexity: 271.795\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6w1shZgupWKq"
      },
      "source": [
        "Best setting: [0.01, 0.29, 0.5, 0.19, 0.01]\n",
        "\n",
        "Lowest Valid Perplexity: 270.364"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QsyZcX5NFD2I"
      },
      "source": [
        "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MXbg0_GLFD2I",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IKRpKvHPFD2K"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0I_ULEaoFD2L",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import RNNCell\n",
        "from torch.nn import RNNBase, RNN\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import Embedding\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2bgOuOZwFD2M"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4Z4ZOHuFD2N",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f-H2LVI9hQNF",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
        "\n",
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    pad_token = wiki_dict.get_id('<pad>')\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5H5Cby2hWMS",
        "outputId": "9df2c530-9378-4ee3-9865-70a7d6648233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "wiki_dict = pkl.load(open(\"wiki_dict.p\", \"rb\"))\n",
        "wiki_tokenized_datasets = tokenize_dataset(datasets, wiki_dict)\n",
        "wiki_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wiki_tokenized_datasets.items():\n",
        "    wiki_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "wiki_loaders = {}\n",
        "batch_size = 32\n",
        "for split, wiki_dataset in wiki_tensor_dataset.items():\n",
        "    wiki_loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 102765.86it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 129278.27it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 127194.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AHb3rdoxCPcH"
      },
      "source": [
        "#### Train RNN model (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "djvpCg40gYw6",
        "colab": {}
      },
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['rnn_dropout'], batch_first=True)\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_outputs = self.rnn(embeddings)\n",
        "        logits = self.projection(rnn_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BdGXmLhxgqNg",
        "colab": {}
      },
      "source": [
        "load_pretrained = False\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "\n",
        "embedding_size = 64\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "rnn_dropout = 0.1\n",
        "\n",
        "options = {\n",
        "    'num_embeddings': len(wiki_dict),\n",
        "    'embedding_dim': embedding_size,\n",
        "    'padding_idx': wiki_dict.get_id('<pad>'),\n",
        "    'input_size': embedding_size,\n",
        "    'hidden_size': hidden_size,\n",
        "    'num_layers': num_layers,\n",
        "    'rnn_dropout': rnn_dropout,\n",
        "}\n",
        "\n",
        "    \n",
        "model = RNNLanguageModel(options).to(current_device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LpG6FnfxiQSB",
        "outputId": "7e023e00-6ae6-4488-ade5-7c6070e792b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "plot_cache = []\n",
        "\n",
        "for epoch_number in range(10):\n",
        "    avg_loss=0\n",
        "    # do train\n",
        "    model.train()\n",
        "    train_log_cache = []\n",
        "    for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
        "        optimizer.zero_grad()\n",
        "        inp = inp.to(current_device)\n",
        "        target = target.to(current_device)\n",
        "        logits = model(inp)\n",
        "\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_log_cache.append(loss.item())\n",
        "\n",
        "    avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "    ppl = 2**(avg_loss/np.log(2))\n",
        "    print('Epoch {} avg train perplexity = {:.{prec}f}'.format(epoch_number+1, ppl, prec=4))\n",
        "    train_log_cache = []\n",
        "            \n",
        "    #do valid\n",
        "    valid_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(wiki_loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_losses.append(loss.item())\n",
        "        avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "        ppl_val = 2**(avg_val_loss/np.log(2))\n",
        "        print('Validation Perplexity after {} epoch = {:.{prec}f}'.format(epoch_number, ppl_val, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "torch.save(model.state_dict(), \"RNN_Baseline.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 avg train perplexity = 520.6948\n",
            "Validation Perplexity after 0 epoch = 300.4343\n",
            "Epoch 1 avg train perplexity = 284.1837\n",
            "Validation Perplexity after 1 epoch = 249.6376\n",
            "Epoch 2 avg train perplexity = 222.1489\n",
            "Validation Perplexity after 2 epoch = 226.3106\n",
            "Epoch 3 avg train perplexity = 188.7494\n",
            "Validation Perplexity after 3 epoch = 217.5824\n",
            "Epoch 4 avg train perplexity = 166.5211\n",
            "Validation Perplexity after 4 epoch = 212.1410\n",
            "Epoch 5 avg train perplexity = 151.0226\n",
            "Validation Perplexity after 5 epoch = 208.0731\n",
            "Epoch 6 avg train perplexity = 139.4528\n",
            "Validation Perplexity after 6 epoch = 207.9464\n",
            "Epoch 7 avg train perplexity = 130.3967\n",
            "Validation Perplexity after 7 epoch = 205.7767\n",
            "Epoch 8 avg train perplexity = 123.0918\n",
            "Validation Perplexity after 8 epoch = 207.6671\n",
            "Epoch 9 avg train perplexity = 116.9712\n",
            "Validation Perplexity after 9 epoch = 207.1735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L__S_AYSDIJE",
        "colab": {}
      },
      "source": [
        "# rnn = RNNLanguageModel(options).to(current_device)\n",
        "# rnn.load_state_dict(torch.load('RNN_Baseline.ckpt'))\n",
        "# valid_losses = []\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     for i, (inp, target) in enumerate(wiki_loaders['valid']):\n",
        "#         inp = inp.to(current_device)\n",
        "#         target = target.to(current_device)\n",
        "#         logits = rnn(inp)\n",
        "\n",
        "#         loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "#         valid_losses.append(loss.item())\n",
        "#     avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "#     ppl_val = 2**(avg_val_loss/np.log(2))\n",
        "#     print('Validation Perplexity = {:.{prec}f}'.format(ppl_val, prec=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "onS5J40EshlG"
      },
      "source": [
        "\n",
        "\n",
        "**Baseline validation perplexity = 207.1735**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dYSozgT-FD2O"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zDvJU1RExG2V"
      },
      "source": [
        "#### Implement LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xZ_UFDZ7FD2P",
        "colab": {}
      },
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], batch_first=True)\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "      \n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        lstm_outputs = self.lstm(embeddings)\n",
        "        logits = self.projection(lstm_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nHkE4Fvanuvr"
      },
      "source": [
        "#### Function for hyper-parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UmTDPW2TEUhG",
        "colab": {}
      },
      "source": [
        "def TuneLSTM(options, learning_rate=0.001, epochs=10, save_model=False):\n",
        "  \n",
        "    model = LSTMLanguageModel(options).to(current_device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'))\n",
        "    model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.Adam(model_parameters, lr=0.001)\n",
        "\n",
        "    ppl, ppl_val = 0, 0\n",
        "    \n",
        "    for epoch_number in range(10):\n",
        "        avg_loss=0\n",
        "        # do train\n",
        "        model.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(wiki_loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_log_cache.append(loss.item())\n",
        "\n",
        "        avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "        ppl = 2**(avg_loss/np.log(2))\n",
        "        if epoch_number == 9:\n",
        "            print('Avg train perplexity = {:.{prec}f}'.format(ppl, prec=4))\n",
        "        train_log_cache = []\n",
        "\n",
        "        #do valid\n",
        "        valid_losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(wiki_loaders['valid']):\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                logits = model(inp)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item())\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            ppl_val = 2**(avg_val_loss/np.log(2))\n",
        "            if epoch_number == 9:\n",
        "                print('Validation Perplexity = {:.{prec}f}'.format(ppl_val, prec=4))\n",
        "                print()\n",
        "    \n",
        "    if save_model:\n",
        "        torch.save(model.state_dict(), \"LSTM_best.ckpt\")\n",
        "        \n",
        "    return ppl, ppl_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uKy5oDovr6Lx"
      },
      "source": [
        "#### Tune learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pVuybPz2p0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "options = {'embedding_dim': 64,\n",
        "           'hidden_size': 128,\n",
        "           'input_size': 64,\n",
        "           'num_embeddings': 33178,\n",
        "           'num_layers': 2,\n",
        "           'padding_idx': 2,\n",
        "           'rnn_dropout': 0.1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GmpypPX1r5uN",
        "outputId": "7a0fa033-6db2-4a67-a99e-189db301dfd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "lr_range = [0.0001, 0.001, 0.005, 0.01, 0.1]\n",
        "ppl_ls, ppl_val_ls = [], []\n",
        "for lr in lr_range:\n",
        "    ppl, ppl_val = TuneLSTM(options, learning_rate=lr, epochs=10)\n",
        "    ppl_ls.append(ppl)\n",
        "    ppl_val_ls.append(ppl_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg train perplexity = 100.5221\n",
            "Validation Perplexity = 203.4454\n",
            "\n",
            "Avg train perplexity = 103.2668\n",
            "Validation Perplexity = 206.7101\n",
            "\n",
            "Avg train perplexity = 104.4677\n",
            "Validation Perplexity = 204.8227\n",
            "\n",
            "Avg train perplexity = 108.9131\n",
            "Validation Perplexity = 209.4359\n",
            "\n",
            "Avg train perplexity = 101.6385\n",
            "Validation Perplexity = 201.7499\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RVJPlgTqaisx",
        "colab": {}
      },
      "source": [
        "# lr_range = [0.0001, 0.001, 0.005, 0.01, 0.1]\n",
        "# ppl_ls = [100.5221, 103.2668, 104.4677, 108.9131, 101.6385]\n",
        "# ppl_val_ls = [203.4454, 206.7101, 204.8227, 209.4359, 201.7499]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lZN7bSiow2zH",
        "outputId": "b676bae2-ace8-4e2a-a92e-48fd3c9c0ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(np.arange(5), ppl_ls, label='Train ppl')\n",
        "plt.plot(np.arange(5), ppl_val_ls, label='Validation ppl')\n",
        "plt.scatter(np.arange(5), ppl_ls)\n",
        "plt.scatter(np.arange(5), ppl_val_ls)\n",
        "plt.xticks(np.arange(5), lr_range)\n",
        "plt.title('Tune Learning Rate')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkkgKEiIhE+v/dKasv2V2o3gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MVbWjWgRc5V-"
      },
      "source": [
        "Best learning rate = 0.1\n",
        "\n",
        "Lowest validation perplexity = 201.7499"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cJeykS5yO7Q"
      },
      "source": [
        "#### Tune number of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_PBXsWF2oB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "options = {'embedding_dim': 64,\n",
        "           'hidden_size': 128,\n",
        "           'input_size': 64,\n",
        "           'num_embeddings': 33178,\n",
        "           'num_layers': 2,\n",
        "           'padding_idx': 2,\n",
        "           'rnn_dropout': 0.1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CTOoGPE6yhG9",
        "outputId": "c8952582-bb07-4e70-db78-eb4117964d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "ppl_ls, ppl_val_ls = [], []\n",
        "num_layers_range = [2, 3, 4]\n",
        "for num_layers in num_layers_range:\n",
        "    options['num_layers'] = num_layers\n",
        "    ppl, ppl_val = TuneLSTM(options, learning_rate=0.1)\n",
        "    ppl_ls.append(ppl)\n",
        "    ppl_val_ls.append(ppl_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg train perplexity = 100.9032\n",
            "Validation Perplexity = 201.2449\n",
            "\n",
            "Avg train perplexity = 116.5239\n",
            "Validation Perplexity = 214.9222\n",
            "\n",
            "Avg train perplexity = 127.5283\n",
            "Validation Perplexity = 229.9399\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k2Q-x5hh2Bde",
        "colab": {}
      },
      "source": [
        "# num_layers_range = [2, 3, 4]\n",
        "# ppl_ls = [100.9032, 116.5239, 127.5283]\n",
        "# ppl_val_ls = [201.2449, 214.9222, 229.9399]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QEOMg_69yhS6",
        "outputId": "b930abed-336c-4daa-d7bb-a3320c1a26b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(np.arange(3), ppl_ls, label='Train ppl')\n",
        "plt.plot(np.arange(3), ppl_val_ls, label='Validation ppl')\n",
        "plt.scatter(np.arange(3), ppl_ls)\n",
        "plt.scatter(np.arange(3), ppl_val_ls)\n",
        "plt.xticks(np.arange(3), num_layers_range)\n",
        "plt.title('Tune Num of Layers')\n",
        "plt.xlabel('Num of Layers')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhcChAiIhKXAoSIiMT1/wEWD8dhH4pz+wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yr8bkvU80Uah"
      },
      "source": [
        "Best Num_of_Layers = 2\n",
        "\n",
        "Lowest validation perplexity = 201.2449"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQMmGNr12VGF",
        "colab_type": "text"
      },
      "source": [
        "#### Tune embedding dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLiN8XdF2cNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "options = {'embedding_dim': 64,\n",
        "           'hidden_size': 128,\n",
        "           'input_size': 64,\n",
        "           'num_embeddings': 33178,\n",
        "           'num_layers': 2,\n",
        "           'padding_idx': 2,\n",
        "           'rnn_dropout': 0.1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAqqplLs2Ybo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_list = [50, 64, 80, 128]\n",
        "ppl_ls, ppl_val_ls = [], []\n",
        "for emb_dim in emb_list:\n",
        "    options['embedding_dim'] = emb_dim\n",
        "    options['input_size'] = emb_dim\n",
        "    ppl, ppl_val = TuneLSTM(options, learning_rate=0.1, epochs=10)\n",
        "    ppl_ls.append(ppl)\n",
        "    ppl_val_ls.append(ppl_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h03du8r2YkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(np.arange(4), ppl_ls, label='Train ppl')\n",
        "plt.plot(np.arange(4), ppl_val_ls, label='Validation ppl')\n",
        "plt.scatter(np.arange(4), ppl_ls)\n",
        "plt.scatter(np.arange(4), ppl_val_ls)\n",
        "plt.xticks(np.arange(4), emb_list)\n",
        "plt.title('Tune Embedding Dimension')\n",
        "plt.xlabel('Embedding Dimension')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PGfcKJInFD2Q"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gHAhniSrFD2R",
        "colab": {}
      },
      "source": [
        "options = {'embedding_dim': 128,\n",
        "           'hidden_size': 128,\n",
        "           'input_size': 128,\n",
        "           'num_embeddings': 33178,\n",
        "           'num_layers': 2,\n",
        "           'padding_idx': 2,\n",
        "           'rnn_dropout': 0.1}\n",
        "\n",
        "ppl, ppl_val = TuneLSTM(options, learning_rate=0.1, save_model=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrFHgjokACEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f92f417-8f10-4771-c7bc-1c653f5dad7b"
      },
      "source": [
        "lstm = LSTMLanguageModel(options).to(current_device)\n",
        "lstm.load_state_dict(torch.load('LSTM_best.ckpt'))\n",
        "valid_losses = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (inp, target) in enumerate(wiki_loaders['valid']):\n",
        "        inp = inp.to(current_device)\n",
        "        target = target.to(current_device)\n",
        "        logits = lstm(inp)\n",
        "\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "        valid_losses.append(loss.item())\n",
        "    avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "    ppl_val = 2**(avg_val_loss/np.log(2))\n",
        "    print('Validation Perplexity = {:.{prec}f}'.format(ppl_val, prec=4))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Perplexity = 199.5804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MtStk2HTFD2T"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UuoRjCbGFD2T",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5dUp76rbFD2V"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bV0DtsWzBLKd"
      },
      "source": [
        "#### Find cloest and furthest words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mCcg65DXBT4r",
        "colab": {}
      },
      "source": [
        "options = {\n",
        "    'num_embeddings': 33178, \n",
        "    'embedding_dim': 64,\n",
        "    'padding_idx': 2,\n",
        "    'input_size': 64,\n",
        "    'hidden_size': 128,\n",
        "    'num_layers': 2,\n",
        "    'rnn_dropout': 0.1,\n",
        "}\n",
        "\n",
        "model = LSTMLanguageModel(options).to(current_device)\n",
        "model.load_state_dict(torch.load('LSTM_best.ckpt'))\n",
        "embed = model.lookup.to('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uq5jKqTnBURd",
        "outputId": "8ce22c5b-4ed9-4feb-90dd-9de6ad8fbb9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "words = ['white', 'cat', 'has', 'short', 'leg']\n",
        "ids = wiki_dict.encode_token_seq(words)\n",
        "vecs = embed(torch.LongTensor(ids)).detach().numpy()\n",
        "all_vecs = embed(torch.LongTensor(np.arange(len(wiki_dict)))).detach().numpy()\n",
        "for i in range(5):\n",
        "    vec = vecs[i]\n",
        "    cos_sim_ls = [sum(vec*v)/np.linalg.norm(vec)*np.linalg.norm(v) for v in all_vecs]\n",
        "    sorted_cos_sim = np.sort(cos_sim_ls)\n",
        "    closest, furthest = [], []\n",
        "    for j in range(10):\n",
        "      closest.append(int(np.where(cos_sim_ls==sorted_cos_sim[-j-2])[0]))\n",
        "      furthest.append(int(np.where(cos_sim_ls==sorted_cos_sim[j])[0]))\n",
        "    \n",
        "    print('10 cloeset words of \"{}\"'.format(words[i]))\n",
        "    print(wiki_dict.decode_idx_seq(closest))\n",
        "    print()\n",
        "    print('10 furthest words of \"{}\"'.format(words[i]))\n",
        "    print(wiki_dict.decode_idx_seq(furthest))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 cloeset words of \"white\"\n",
            "['survivors', 'wanted', 'Writers', 'Industry', 'Lopez', 'Ophelia', 'tawny', 'turkey', 'Reid', 'beautifully']\n",
            "\n",
            "10 furthest words of \"white\"\n",
            "['born', 'Twins', '1859', 'unwelcome', 'beginning', 'set', 'Quahog', 'tonic', 'remarked', 'Mac']\n",
            "\n",
            "10 cloeset words of \"cat\"\n",
            "['fact', 'Poles', 'bloodstream', 'cat', 'grenadiers', 'submarines', 'contamination', 'engineered', 'clades', 'Ostend']\n",
            "\n",
            "10 furthest words of \"cat\"\n",
            "['becoming', 'spend', 'late', 'Citra', 'possibly', 'May', 'Inter', 'Marquette', 'number', 'unpleasant']\n",
            "\n",
            "10 cloeset words of \"has\"\n",
            "['have', 'helicopter', 'Danny', 'Waterfall', 'Edie', 'Super', 'didn', 'had', 'likely', 'rather']\n",
            "\n",
            "10 furthest words of \"has\"\n",
            "['<unk', '<', '.', '?', 'Therefore', 'roots', 'Relics', 'reels', 'views', 'millimeter']\n",
            "\n",
            "10 cloeset words of \"short\"\n",
            "['south', 'Lewis', 'similar', 'Enterprises', 'spite', 'fake', 'any', 'comedy', '1220', 'Waterfall']\n",
            "\n",
            "10 furthest words of \"short\"\n",
            "['nakedness', 'vacancy', 'Swamp', 'legibility', 'guns', 'dictatorships', '1797', 'safely', '1685', 'Symposium']\n",
            "\n",
            "10 cloeset words of \"leg\"\n",
            "[\"'Automobile\", 'peaking', 'accelerate', 'movable', 'gotta', 'hypothetical', 'video', 'aforementioned', 'recaptured', 'ultra']\n",
            "\n",
            "10 furthest words of \"leg\"\n",
            "['remarking', 'Without', 'there', 'triumphed', 'willing', 'bowlers', 'contrast', 'eroded', 'concerned', 'Turner']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hEPytwW7FD2V"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JEZpRBmjFD2W",
        "outputId": "cc81cd68-1f7d-4e8d-b6ec-b6cfe85e988b",
        "colab": {}
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QNBmEuA9FD2Y",
        "outputId": "443f6eea-071d-46bc-d5dc-e4f2eb7dcf85",
        "colab": {}
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BnFZ2mWrFD2a"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QDY1LxZjFD2a",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b9yLPROjFD2c"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ajV_nE3FD2c",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HnLN38OVFD2e"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8w3jcuqWFD2e",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FzzpebFkFD2h"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RhQmqQXAFD2i"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "27_vDpScFD2i",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oop65tFhFD2l"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XPsJQM-bFD2m",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P4MgROaRFD2o"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ftL6eIPwFD2p",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pDvxYZBtFD2r"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AFSKZbkOFD2s",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zAnPOZzvFD2x"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hKKkDVtkFD2x",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "owgPfDU3FD2z"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iaQZig44FD2z",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}